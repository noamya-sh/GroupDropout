{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuClass":"premium","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'CV7062610/assignments/assignment3/'\n","FOLDERNAME = 'Project'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/My\\ Drive/$FOLDERNAME\n","# !bash get_datasets.sh\n","# %cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEKJcPjlw28x","executionInfo":{"status":"ok","timestamp":1685898293367,"user_tz":-180,"elapsed":3785,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"outputId":"9f92534d-bcf8-40cf-bd3e-d2eee4301e5b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1ZiVkCzDJSBJ07iUM9ppV1p4ONhFIeZB-/Project\n"]}]},{"cell_type":"code","source":["\n","# Import libraries\n","\n","import torch\n","import time\n","import copy\n","import os\n","import torchvision\n","import torch.nn as nn\n","from torch import optim\n","import tensorflow as tf\n","import torch.nn.functional as F\n","from torchvision import datasets\n","from torchsummary import summary\n","from torch.autograd import Variable\n","from torchvision.transforms import ToTensor\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset"],"metadata":{"id":"bbiBXgWcPVqc","executionInfo":{"status":"ok","timestamp":1685898303951,"user_tz":-180,"elapsed":8629,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **Data - CiFAR10**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"O0fRsXUlTTK2"}},{"cell_type":"code","source":["# Image preprocessing modules\n","transform = transforms.Compose([\n","    transforms.Pad(4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32),\n","    transforms.ToTensor()])"],"metadata":{"id":"C-cSidC7TNqi","executionInfo":{"status":"ok","timestamp":1685898303951,"user_tz":-180,"elapsed":3,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n","                                             train=True, \n","                                             transform=transform,\n","                                             download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n","                                            train=False, \n","                                            transform=transforms.ToTensor())"],"metadata":{"id":"ja8NuFgUTNtv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685898310528,"user_tz":-180,"elapsed":6579,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"outputId":"d4adf42d-3f76-4c02-dbca-5eac082d574f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=100, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=100, \n","                                          shuffle=False)"],"metadata":{"id":"tOA5AWZiTNwg","executionInfo":{"status":"ok","timestamp":1685898310528,"user_tz":-180,"elapsed":20,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"7HvZwrJePWW0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685898310529,"user_tz":-180,"elapsed":16,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"outputId":"eff6a754-8e5f-4365-e9c6-491a286faf9d"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["torch.cuda.get_device_name()"],"metadata":{"id":"rp8UoFFNzctj","executionInfo":{"status":"ok","timestamp":1685898310529,"user_tz":-180,"elapsed":13,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"91087fbe-d0ae-4e20-9fbb-2a5555558e2a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# **Model**\n","\n","---\n","\n"],"metadata":{"id":"Ssv9OpeSTrPF"}},{"cell_type":"code","source":["def weights_init(module):\n","  if isinstance(module, nn.Linear):\n","    random_seed = 1\n","    torch.manual_seed(random_seed)\n","    nn.init.normal_(module.weight, mean=0, std=1.0)\n","    \n","    if module.bias is not None:\n","      nn.init.constant_(module.bias, 0) "],"metadata":{"id":"xU8ZEzZ1EAGa","executionInfo":{"status":"ok","timestamp":1685898310531,"user_tz":-180,"elapsed":13,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class ResBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, downsample):\n","        super().__init__()\n","        if downsample:\n","            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","        else:\n","            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","            self.shortcut = nn.Sequential()\n","\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","    def forward(self, input):\n","        shortcut = self.shortcut(input)\n","        input = nn.ReLU()(self.bn1(self.conv1(input)))\n","        input = nn.ReLU()(self.bn2(self.conv2(input)))\n","        input = input + shortcut\n","        return nn.ReLU()(input)"],"metadata":{"id":"PsKLvvaMhD4E","executionInfo":{"status":"ok","timestamp":1685898310531,"user_tz":-180,"elapsed":12,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class CustomDropout(nn.Module):\n","    \"\"\"\n","    :parameter\n","    p: probability to drop. Bigger p -> Drop more\n","    \"\"\"\n","    def __init__(self, num_of_labels, p=0.5):\n","        super(CustomDropout, self).__init__()\n","        self.p = p\n","        self.num_of_labels = num_of_labels\n","\n","    def __repr__(self):\n","        return  f\"CustomDropout(num_of_labels={self.num_of_labels}, p={self.p})\"\n","\n","    def forward(self, batch_input, batch_labels):\n","        if self.training:\n","            layer_size = batch_input.size(1)\n","            batch_size = batch_input.size(0)\n","            portion_size = int(layer_size // self.num_of_labels)\n","            # print(f\"portion_size: {portion_size}\")\n","            # print(batch_input)\n","\n","            for i in range(batch_size):\n","                label = batch_labels[i].item()\n","                # print(f\"{label} * {portion_size} = {label * portion_size}\")\n","                # print(f\"{label+1} * {portion_size} = {(label + 1) * portion_size}\")\n","                mask = torch.bernoulli(torch.ones_like(batch_input[i]) * (1-self.p))\n","                # mask = torch.zeros_like()\n","                # print(label * portion_size, \"-\", (label + 1) * portion_size)\n","                # print((1.0 + (1/portion_size)))\n","                mask[label * portion_size : (label + 1) * portion_size] = 1.0\n","                batch_input[i] = batch_input[i] * mask\n","                if self.p == 1:\n","                    # in case that drop all neurons not from the label group\n","                    power_for_left_neurons = self.num_of_labels\n","                else:\n","                     # in case that drop anything -> power=1\n","                    power_for_left_neurons = (1/(1-self.p)) if self.p != 0 else 1\n","\n","                # give each neuron power of the dropped neurons\n","                batch_input[i] *= power_for_left_neurons\n","\n","            output = batch_input\n","        else:\n","            output = batch_input\n","\n","        return output"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"AjS2qn6YwYQL","executionInfo":{"status":"ok","timestamp":1685898310532,"user_tz":-180,"elapsed":12,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}}},{"cell_type":"markdown","source":["## **ResNet**\n","\n","---\n","\n"],"metadata":{"id":"misrssGbpDai"}},{"cell_type":"code","source":["class ResNet(nn.Module):\n","    def __init__(self, in_channels, resblock, repeat, outputs=1000, DROPOUT=True):\n","        super().__init__()\n","        self.layer0 = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","\n","        filters = [64, 64, 128, 256, 512]\n","\n","        self.layer1 = nn.Sequential()\n","        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n","        for i in range(1, repeat[0]):\n","                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n","\n","        self.layer2 = nn.Sequential()\n","        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n","        for i in range(1, repeat[1]):\n","                self.layer2.add_module('conv3_%d' % (i+1,), resblock(filters[2], filters[2], downsample=False))\n","\n","        self.layer3 = nn.Sequential()\n","        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n","        for i in range(1, repeat[2]):\n","            self.layer3.add_module('conv2_%d' % (i+1,), resblock(filters[3], filters[3], downsample=False))\n","\n","        self.layer4 = nn.Sequential()\n","        self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n","        for i in range(1, repeat[3]):\n","            self.layer4.add_module('conv3_%d'%(i+1,), resblock(filters[4], filters[4], downsample=False))\n","\n","        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n","        self.fc = torch.nn.Linear(filters[4], outputs)\n","        self.dropout = CustomDropout(num_of_labels=10, p=0.6) if DROPOUT else None\n","        if DROPOUT:\n","            print(self.dropout)\n","        # self.dropout = nn.Dropout(p=0.5) if DROPOUT else None\n","\n","    def forward(self, input, input_labels):\n","        input = self.layer0(input)\n","        input = self.layer1(input)\n","        input = self.layer2(input)\n","        input = self.layer3(input)\n","        input = self.layer4(input)\n","        input = self.gap(input)\n","        input = torch.flatten(input, start_dim=1)\n","        input = self.fc(input)\n","        if self.dropout is not None:\n","            input = self.dropout(input, input_labels)\n","\n","        return input"],"metadata":{"id":"1tau2zWXpHc5","executionInfo":{"status":"ok","timestamp":1685898322213,"user_tz":-180,"elapsed":326,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# **Train & Test**"],"metadata":{"id":"OsPUbgOPiapy"}},{"cell_type":"code","source":["# define everything we need for training\n","\n","epochs = 200 #1\n","criterion = nn.CrossEntropyLoss()\n","\n","# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n","# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"],"metadata":{"id":"a8MotdABhUs-","executionInfo":{"status":"ok","timestamp":1685898329772,"user_tz":-180,"elapsed":318,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# count trainable parameters of the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"metadata":{"id":"rlxQaNC2Nc-z","executionInfo":{"status":"ok","timestamp":1685898333884,"user_tz":-180,"elapsed":3,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=50, is_inception=False):\n","    \n","    since = time.time()\n","    val_acc_history = []\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    # if os.path.exists(\"/content/drive/MyDrive/models/model.pth\"):\n","    #     model.load_state_dict(torch.load('/content/drive/MyDrive/model/model.pth'))\n","    #     print(\"Load previes model\")\n","\n","    for epoch in range(num_epochs):\n","        try:\n","          if epoch==100:\n","              print('Best test Acc: {:4f}%'.format(best_acc*100))\n","        except Exception as e:\n","              print(f\"Error while print ACC at epoch 100:\\n{e}\")\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'test']: # Each epoch has a training and validation phase\n","            if phase == 'train':\n","                model.train()           # Set model to training mode\n","            else:\n","                model.eval()            # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            # i=0\n","\n","            for inputs, labels in dataloaders[phase]: # Iterate over data\n","                \n","                inputs = transforms.functional.resize(inputs, (112, 112))\n","                inputs = inputs.to(device)\n","\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad() # Zero the parameter gradients\n","\n","                with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n","                    outputs = model(inputs, labels)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    if phase == 'train': # Backward + optimize only if in training phase\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # Statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                # i += 1\n","                # if i%30==0:\n","                #   print(loss.item())\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            \n","            if phase == 'test': # Adjust learning rate based on val loss\n","                lr_scheduler.step(epoch_loss)\n","                \n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print(\"-------------------------------------------\")\n","            print('{}\\t Loss: {:.4f}\\t Acc: {:.4f}%'.format(phase, epoch_loss, epoch_acc*100))\n","            print(\"-------------------------------------------\\n\")\n","            \n","            if phase == 'test':\n","                val_acc_history.append(epoch_acc)\n","\n","                # deep copy the model\n","                if epoch_acc > best_acc:\n","                  best_acc = epoch_acc\n","                  # Save the trained model each epoch\n","                  file_save = f'/content/drive/MyDrive/models/model.pth'\n","                  torch.save(model.state_dict(), file_save)\n","                  print(f\"\\n\\t---> Saved model in epoch: {epoch}\\n\")\n","                  # best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        # print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best test Acc: {:4f}%'.format(best_acc*100))\n","\n","    # # load best model weights\n","    # model.load_state_dict(best_model_wts)\n","\n","    return model, val_acc_history"],"metadata":{"id":"xTrfEt5LhZrM","executionInfo":{"status":"ok","timestamp":1685898689856,"user_tz":-180,"elapsed":343,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## **Train & Test Models**\n","\n","---\n","\n"],"metadata":{"id":"j9m2mOwJnmNf"}},{"cell_type":"markdown","source":["### **ResNet34 - Group Dropout**\n","\n","---\n","\n"],"metadata":{"collapsed":false,"pycharm":{"name":"#%% md\n"},"id":"4QvoPEskwYQy"}},{"cell_type":"code","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["CustomDropout(num_of_labels=10, p=0.6)\n","Load old weight\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (layer0): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n","    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): ReLU()\n","  )\n","  (layer1): Sequential(\n","    (conv2_1): ResBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_2): ResBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_3): ResBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (conv3_1): ResBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3_2): ResBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3_3): ResBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3_4): ResBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (conv4_1): ResBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_2): ResBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_3): ResBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_4): ResBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_5): ResBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv2_6): ResBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (conv5_1): ResBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3_2): ResBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3_3): ResBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (shortcut): Sequential()\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (gap): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n","  (dropout): CustomDropout(num_of_labels=10, p=0.6)\n",")"]},"metadata":{},"execution_count":22}],"source":["# resnet34 - dropout\n","weight_path = '/content/drive/MyDrive/Project/model_06_init.pth'\n","res34_d = ResNet(3, ResBlock, [3, 4, 6, 3], outputs=1000, DROPOUT=True)\n","\n","# in order to load new wights:\n","# res34_d.apply(weights_init)\n","# torch.save(res34_d.state_dict(), weight_path)\n","\n","\n","# in order to load init weight:\n","res34_d.load_state_dict(torch.load(weight_path)) \n","print (\"Load old weight\")\n","\n","res34_d.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n","\n","\n","#Todo: maybe change number of cuda"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpWaklgGwYQz","executionInfo":{"status":"ok","timestamp":1685898692570,"user_tz":-180,"elapsed":359,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"outputId":"1ef5b494-0462-47aa-cf2c-f30393ca2dfb"}},{"cell_type":"code","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21806184"]},"metadata":{},"execution_count":23}],"source":["count_parameters(res34_d) # 21,806,184"],"metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"uOMRhIAYwYQ0","executionInfo":{"status":"ok","timestamp":1685898692570,"user_tz":-180,"elapsed":14,"user":{"displayName":"project2 yosiElias","userId":"03638168708983838192"}},"outputId":"67e8eddc-070c-46e4-dd4d-a220086cfd5c"}},{"cell_type":"markdown","source":["# drop = 0.6; epoch = 200; init weight = 6_init"],"metadata":{"id":"JQttiqgKRwZ6"}},{"cell_type":"code","source":["optimizer = optim.Adam(res34_d.parameters(), lr=0.0001, weight_decay=1e-4)\n","lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n","\n","res34_d, _ = train_model(res34_d,\n","                         {\"train\": train_loader, \"test\": test_loader},\n","                         criterion,\n","                         optimizer,\n","                         epochs)"],"metadata":{"id":"xKCTrvuqKHCC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8efcb8a-2f67-4e5b-f693-f7f09d08a2f1","pycharm":{"is_executing":true}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/199\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["-------------------------------------------\n","train\t Loss: 18.1529\t Acc: 15.7960%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.4462\t Acc: 23.3400%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 0\n","\n","Epoch 1/199\n","----------\n","-------------------------------------------\n","train\t Loss: 8.3311\t Acc: 22.3160%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.8722\t Acc: 34.8400%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 1\n","\n","Epoch 2/199\n","----------\n","-------------------------------------------\n","train\t Loss: 6.4051\t Acc: 25.2660%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 6.1516\t Acc: 25.0700%\n","-------------------------------------------\n","\n","Epoch 3/199\n","----------\n","-------------------------------------------\n","train\t Loss: 5.6967\t Acc: 26.6120%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.3361\t Acc: 35.4100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 3\n","\n","Epoch 4/199\n","----------\n","-------------------------------------------\n","train\t Loss: 5.5659\t Acc: 26.4100%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.4317\t Acc: 42.5100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 4\n","\n","Epoch 5/199\n","----------\n","-------------------------------------------\n","train\t Loss: 5.0876\t Acc: 27.4340%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.4823\t Acc: 44.1500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 5\n","\n","Epoch 6/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.8672\t Acc: 28.8780%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.6724\t Acc: 45.0500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 6\n","\n","Epoch 7/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.9304\t Acc: 27.6880%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.3481\t Acc: 41.4200%\n","-------------------------------------------\n","\n","Epoch 8/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.6288\t Acc: 29.1760%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.6616\t Acc: 49.0700%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 8\n","\n","Epoch 9/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.5193\t Acc: 30.2660%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.3003\t Acc: 52.8300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 9\n","\n","Epoch 10/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.3566\t Acc: 31.4880%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.3182\t Acc: 55.2500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 10\n","\n","Epoch 11/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.3219\t Acc: 32.3300%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.1749\t Acc: 54.9500%\n","-------------------------------------------\n","\n","Epoch 12/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2879\t Acc: 33.1740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.2337\t Acc: 58.1600%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 12\n","\n","Epoch 13/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2751\t Acc: 33.5680%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.9484\t Acc: 60.0200%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 13\n","\n","Epoch 14/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2530\t Acc: 34.3420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.0411\t Acc: 63.1200%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 14\n","\n","Epoch 15/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2352\t Acc: 34.9020%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.1860\t Acc: 63.6900%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 15\n","\n","Epoch 16/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.1679\t Acc: 35.8000%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.9295\t Acc: 65.7000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 16\n","\n","Epoch 17/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.1187\t Acc: 36.8260%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.9536\t Acc: 58.6900%\n","-------------------------------------------\n","\n","Epoch 18/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.1880\t Acc: 36.2920%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.2309\t Acc: 40.0300%\n","-------------------------------------------\n","\n","Epoch 19/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.3322\t Acc: 33.6360%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 3.2777\t Acc: 58.7600%\n","-------------------------------------------\n","\n","Epoch 20/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2066\t Acc: 35.7040%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.9452\t Acc: 64.3100%\n","-------------------------------------------\n","\n","Epoch 21/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.0976\t Acc: 37.4580%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 4.6454\t Acc: 64.1500%\n","-------------------------------------------\n","\n","Epoch 22/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.0821\t Acc: 37.6900%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.7771\t Acc: 64.7700%\n","-------------------------------------------\n","\n","Epoch 23/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9725\t Acc: 38.5660%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.3344\t Acc: 73.7300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 23\n","\n","Epoch 24/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9747\t Acc: 39.1440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.7267\t Acc: 67.0400%\n","-------------------------------------------\n","\n","Epoch 25/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.0595\t Acc: 38.0780%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.3593\t Acc: 67.8100%\n","-------------------------------------------\n","\n","Epoch 26/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.2221\t Acc: 36.3720%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.4406\t Acc: 69.6100%\n","-------------------------------------------\n","\n","Epoch 27/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9908\t Acc: 38.9740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.5215\t Acc: 70.6000%\n","-------------------------------------------\n","\n","Epoch 28/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9275\t Acc: 40.1820%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.2881\t Acc: 75.6500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 28\n","\n","Epoch 29/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9280\t Acc: 40.4460%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.3419\t Acc: 75.3400%\n","-------------------------------------------\n","\n","Epoch 30/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9392\t Acc: 40.4460%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.5887\t Acc: 69.8100%\n","-------------------------------------------\n","\n","Epoch 31/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.9178\t Acc: 40.5600%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.6646\t Acc: 74.7400%\n","-------------------------------------------\n","\n","Epoch 32/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8658\t Acc: 41.4200%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.4395\t Acc: 74.9500%\n","-------------------------------------------\n","\n","Epoch 33/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8680\t Acc: 41.4380%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.0933\t Acc: 76.4300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 33\n","\n","Epoch 34/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8990\t Acc: 41.1300%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.1151\t Acc: 78.0600%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 34\n","\n","Epoch 35/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8712\t Acc: 41.5840%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.0369\t Acc: 77.3900%\n","-------------------------------------------\n","\n","Epoch 36/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.0819\t Acc: 38.4220%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.6287\t Acc: 66.4900%\n","-------------------------------------------\n","\n","Epoch 37/199\n","----------\n","-------------------------------------------\n","train\t Loss: 4.0280\t Acc: 38.6060%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.2016\t Acc: 73.5700%\n","-------------------------------------------\n","\n","Epoch 38/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8731\t Acc: 41.0560%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.0488\t Acc: 77.8500%\n","-------------------------------------------\n","\n","Epoch 39/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8484\t Acc: 41.7100%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.3518\t Acc: 76.5000%\n","-------------------------------------------\n","\n","Epoch 40/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8138\t Acc: 42.5740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.8563\t Acc: 80.4100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 40\n","\n","Epoch 41/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.8120\t Acc: 42.5420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.7586\t Acc: 80.8000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 41\n","\n","Epoch 42/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7956\t Acc: 42.8180%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.7631\t Acc: 80.7500%\n","-------------------------------------------\n","\n","Epoch 43/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7947\t Acc: 42.8080%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.8166\t Acc: 79.7900%\n","-------------------------------------------\n","\n","Epoch 44/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7733\t Acc: 43.3100%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.0443\t Acc: 80.6400%\n","-------------------------------------------\n","\n","Epoch 45/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7800\t Acc: 43.2420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 2.3434\t Acc: 78.2200%\n","-------------------------------------------\n","\n","Epoch 46/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7570\t Acc: 43.4500%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.7662\t Acc: 82.1100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 46\n","\n","Epoch 47/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.7564\t Acc: 43.6180%\n","-------------------------------------------\n","\n","Epoch 00048: reducing learning rate of group 0 to 1.0000e-05.\n","-------------------------------------------\n","test\t Loss: 1.8480\t Acc: 81.4300%\n","-------------------------------------------\n","\n","Epoch 48/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6500\t Acc: 44.9120%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.5914\t Acc: 84.8100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 48\n","\n","Epoch 49/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6457\t Acc: 45.0620%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.5310\t Acc: 85.2100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 49\n","\n","Epoch 50/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6519\t Acc: 45.0600%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.5206\t Acc: 85.6200%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 50\n","\n","Epoch 51/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6297\t Acc: 45.3680%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.5025\t Acc: 85.8800%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 51\n","\n","Epoch 52/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6180\t Acc: 45.6180%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.4033\t Acc: 85.9100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 52\n","\n","Epoch 53/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6242\t Acc: 45.3680%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.4010\t Acc: 86.2300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 53\n","\n","Epoch 54/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6167\t Acc: 45.7220%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.4008\t Acc: 86.1400%\n","-------------------------------------------\n","\n","Epoch 55/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6067\t Acc: 45.7400%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.3465\t Acc: 86.4000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 55\n","\n","Epoch 56/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5856\t Acc: 46.1540%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.3697\t Acc: 86.1500%\n","-------------------------------------------\n","\n","Epoch 57/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6235\t Acc: 45.6700%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.3580\t Acc: 86.2900%\n","-------------------------------------------\n","\n","Epoch 58/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5830\t Acc: 46.0760%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.3299\t Acc: 86.4700%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 58\n","\n","Epoch 59/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5870\t Acc: 46.0680%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.2555\t Acc: 86.9000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 59\n","\n","Epoch 60/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5880\t Acc: 46.2020%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.2313\t Acc: 86.7900%\n","-------------------------------------------\n","\n","Epoch 61/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5706\t Acc: 46.4300%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.2681\t Acc: 86.6400%\n","-------------------------------------------\n","\n","Epoch 62/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.6080\t Acc: 45.9440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.2181\t Acc: 86.7900%\n","-------------------------------------------\n","\n","Epoch 63/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5681\t Acc: 46.6100%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.2665\t Acc: 86.7100%\n","-------------------------------------------\n","\n","Epoch 64/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5843\t Acc: 46.2900%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1770\t Acc: 87.0000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 64\n","\n","Epoch 65/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5763\t Acc: 46.3140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1860\t Acc: 86.9800%\n","-------------------------------------------\n","\n","Epoch 66/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5921\t Acc: 46.2140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1255\t Acc: 86.9700%\n","-------------------------------------------\n","\n","Epoch 67/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5699\t Acc: 46.5420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1661\t Acc: 87.0700%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 67\n","\n","Epoch 68/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5709\t Acc: 46.5600%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1803\t Acc: 86.8200%\n","-------------------------------------------\n","\n","Epoch 69/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5458\t Acc: 47.1160%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1435\t Acc: 86.9500%\n","-------------------------------------------\n","\n","Epoch 70/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5495\t Acc: 46.8500%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1465\t Acc: 87.2000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 70\n","\n","Epoch 71/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5432\t Acc: 46.9620%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0917\t Acc: 87.2800%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 71\n","\n","Epoch 72/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5617\t Acc: 46.8500%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1106\t Acc: 87.0600%\n","-------------------------------------------\n","\n","Epoch 73/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5766\t Acc: 46.5700%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0989\t Acc: 87.5300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 73\n","\n","Epoch 74/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5812\t Acc: 46.5240%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.1302\t Acc: 87.5900%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 74\n","\n","Epoch 75/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5609\t Acc: 46.7420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0746\t Acc: 87.3300%\n","-------------------------------------------\n","\n","Epoch 76/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5577\t Acc: 46.9740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0915\t Acc: 87.3800%\n","-------------------------------------------\n","\n","Epoch 77/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5501\t Acc: 47.0840%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0765\t Acc: 87.1500%\n","-------------------------------------------\n","\n","Epoch 78/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5439\t Acc: 47.0920%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0156\t Acc: 87.3500%\n","-------------------------------------------\n","\n","Epoch 79/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5320\t Acc: 47.2060%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9857\t Acc: 87.5500%\n","-------------------------------------------\n","\n","Epoch 80/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5321\t Acc: 47.2820%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0475\t Acc: 87.4100%\n","-------------------------------------------\n","\n","Epoch 81/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5351\t Acc: 47.2140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0166\t Acc: 87.5700%\n","-------------------------------------------\n","\n","Epoch 82/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5577\t Acc: 46.8980%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9794\t Acc: 87.3400%\n","-------------------------------------------\n","\n","Epoch 83/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5245\t Acc: 47.3480%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9929\t Acc: 87.5800%\n","-------------------------------------------\n","\n","Epoch 84/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5385\t Acc: 47.2980%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9795\t Acc: 87.9500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 84\n","\n","Epoch 85/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5418\t Acc: 47.1640%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0060\t Acc: 87.7100%\n","-------------------------------------------\n","\n","Epoch 86/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5255\t Acc: 47.5360%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9536\t Acc: 87.7500%\n","-------------------------------------------\n","\n","Epoch 87/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5116\t Acc: 47.7300%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9716\t Acc: 87.7500%\n","-------------------------------------------\n","\n","Epoch 88/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5316\t Acc: 47.3400%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9756\t Acc: 87.5900%\n","-------------------------------------------\n","\n","Epoch 89/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5205\t Acc: 47.5780%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9674\t Acc: 87.6600%\n","-------------------------------------------\n","\n","Epoch 90/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5151\t Acc: 47.7420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9606\t Acc: 87.5700%\n","-------------------------------------------\n","\n","Epoch 91/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5128\t Acc: 47.7140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 1.0122\t Acc: 87.7000%\n","-------------------------------------------\n","\n","Epoch 92/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5449\t Acc: 47.2460%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9363\t Acc: 87.6600%\n","-------------------------------------------\n","\n","Epoch 93/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5024\t Acc: 47.7920%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9467\t Acc: 87.7300%\n","-------------------------------------------\n","\n","Epoch 94/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5115\t Acc: 47.8440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9496\t Acc: 87.7500%\n","-------------------------------------------\n","\n","Epoch 95/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5106\t Acc: 47.6720%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9089\t Acc: 87.8300%\n","-------------------------------------------\n","\n","Epoch 96/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5080\t Acc: 47.7240%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9231\t Acc: 87.9200%\n","-------------------------------------------\n","\n","Epoch 97/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5184\t Acc: 47.5880%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8817\t Acc: 88.1800%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 97\n","\n","Epoch 98/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5143\t Acc: 47.6860%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8877\t Acc: 88.0100%\n","-------------------------------------------\n","\n","Epoch 99/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4865\t Acc: 48.1480%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9399\t Acc: 87.8600%\n","-------------------------------------------\n","\n","Best test Acc: 88.180000%\n","Epoch 100/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4983\t Acc: 48.0040%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.9191\t Acc: 88.0900%\n","-------------------------------------------\n","\n","Epoch 101/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5095\t Acc: 47.8420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8710\t Acc: 87.7800%\n","-------------------------------------------\n","\n","Epoch 102/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4985\t Acc: 48.0000%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8551\t Acc: 87.8600%\n","-------------------------------------------\n","\n","Epoch 103/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5184\t Acc: 47.7400%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8709\t Acc: 87.9700%\n","-------------------------------------------\n","\n","Epoch 104/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4959\t Acc: 47.9820%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8866\t Acc: 88.1000%\n","-------------------------------------------\n","\n","Epoch 105/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4740\t Acc: 48.2840%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8584\t Acc: 87.9300%\n","-------------------------------------------\n","\n","Epoch 106/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.5050\t Acc: 47.8300%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8942\t Acc: 87.9400%\n","-------------------------------------------\n","\n","Epoch 107/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4873\t Acc: 48.1160%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8838\t Acc: 88.0600%\n","-------------------------------------------\n","\n","Epoch 108/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4818\t Acc: 48.2580%\n","-------------------------------------------\n","\n","Epoch 00109: reducing learning rate of group 0 to 1.0000e-06.\n","-------------------------------------------\n","test\t Loss: 0.8723\t Acc: 87.9100%\n","-------------------------------------------\n","\n","Epoch 109/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4622\t Acc: 48.4140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8459\t Acc: 88.4300%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 109\n","\n","Epoch 110/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4642\t Acc: 48.3900%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8064\t Acc: 88.5100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 110\n","\n","Epoch 111/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4587\t Acc: 48.6080%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8063\t Acc: 88.5800%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 111\n","\n","Epoch 112/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4865\t Acc: 48.1800%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8374\t Acc: 88.5500%\n","-------------------------------------------\n","\n","Epoch 113/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4488\t Acc: 48.5940%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8010\t Acc: 88.5200%\n","-------------------------------------------\n","\n","Epoch 114/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4732\t Acc: 48.2780%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8280\t Acc: 88.6100%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 114\n","\n","Epoch 115/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4611\t Acc: 48.4700%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8096\t Acc: 88.4900%\n","-------------------------------------------\n","\n","Epoch 116/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4443\t Acc: 48.7140%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7948\t Acc: 88.6100%\n","-------------------------------------------\n","\n","Epoch 117/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4762\t Acc: 48.3500%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8160\t Acc: 88.5300%\n","-------------------------------------------\n","\n","Epoch 118/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4761\t Acc: 48.2440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8165\t Acc: 88.5200%\n","-------------------------------------------\n","\n","Epoch 119/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4448\t Acc: 48.8800%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7997\t Acc: 88.4500%\n","-------------------------------------------\n","\n","Epoch 120/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4421\t Acc: 48.7940%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8145\t Acc: 88.5200%\n","-------------------------------------------\n","\n","Epoch 121/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4568\t Acc: 48.5800%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7936\t Acc: 88.5700%\n","-------------------------------------------\n","\n","Epoch 122/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4618\t Acc: 48.5740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8063\t Acc: 88.5100%\n","-------------------------------------------\n","\n","Epoch 123/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4484\t Acc: 48.8380%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7976\t Acc: 88.6600%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 123\n","\n","Epoch 124/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4810\t Acc: 48.2060%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8060\t Acc: 88.5700%\n","-------------------------------------------\n","\n","Epoch 125/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4713\t Acc: 48.5060%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8095\t Acc: 88.5200%\n","-------------------------------------------\n","\n","Epoch 126/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4765\t Acc: 48.2820%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8003\t Acc: 88.5000%\n","-------------------------------------------\n","\n","Epoch 127/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4646\t Acc: 48.6740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7886\t Acc: 88.6100%\n","-------------------------------------------\n","\n","Epoch 128/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4678\t Acc: 48.4960%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8190\t Acc: 88.6200%\n","-------------------------------------------\n","\n","Epoch 129/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4619\t Acc: 48.6800%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7920\t Acc: 88.6000%\n","-------------------------------------------\n","\n","Epoch 130/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4753\t Acc: 48.3120%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7969\t Acc: 88.6200%\n","-------------------------------------------\n","\n","Epoch 131/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4525\t Acc: 48.8040%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7956\t Acc: 88.6000%\n","-------------------------------------------\n","\n","Epoch 132/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4597\t Acc: 48.6560%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7834\t Acc: 88.5200%\n","-------------------------------------------\n","\n","Epoch 133/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4559\t Acc: 48.6380%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.8062\t Acc: 88.6200%\n","-------------------------------------------\n","\n","Epoch 134/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4494\t Acc: 48.7520%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7903\t Acc: 88.6700%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 134\n","\n","Epoch 135/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4760\t Acc: 48.4520%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7994\t Acc: 88.5000%\n","-------------------------------------------\n","\n","Epoch 136/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4513\t Acc: 48.6960%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7854\t Acc: 88.6600%\n","-------------------------------------------\n","\n","Epoch 137/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4627\t Acc: 48.6680%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7851\t Acc: 88.5000%\n","-------------------------------------------\n","\n","Epoch 138/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4405\t Acc: 48.9440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7753\t Acc: 88.6200%\n","-------------------------------------------\n","\n","Epoch 139/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4631\t Acc: 48.5220%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7842\t Acc: 88.4700%\n","-------------------------------------------\n","\n","Epoch 140/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4645\t Acc: 48.5100%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7876\t Acc: 88.6300%\n","-------------------------------------------\n","\n","Epoch 141/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4676\t Acc: 48.4440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7900\t Acc: 88.6100%\n","-------------------------------------------\n","\n","Epoch 142/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4486\t Acc: 48.7740%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7992\t Acc: 88.5500%\n","-------------------------------------------\n","\n","Epoch 143/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4429\t Acc: 48.8440%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7954\t Acc: 88.6900%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 143\n","\n","Epoch 144/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4580\t Acc: 48.8000%\n","-------------------------------------------\n","\n","Epoch 00145: reducing learning rate of group 0 to 1.0000e-07.\n","-------------------------------------------\n","test\t Loss: 0.8003\t Acc: 88.6800%\n","-------------------------------------------\n","\n","Epoch 145/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4424\t Acc: 48.8640%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7854\t Acc: 88.7000%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 145\n","\n","Epoch 146/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4585\t Acc: 48.6240%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7932\t Acc: 88.7500%\n","-------------------------------------------\n","\n","\n","\t---> Saved model in epoch: 146\n","\n","Epoch 147/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4362\t Acc: 48.9420%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7736\t Acc: 88.7000%\n","-------------------------------------------\n","\n","Epoch 148/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4463\t Acc: 48.8880%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7746\t Acc: 88.6500%\n","-------------------------------------------\n","\n","Epoch 149/199\n","----------\n","-------------------------------------------\n","train\t Loss: 3.4615\t Acc: 48.7060%\n","-------------------------------------------\n","\n","-------------------------------------------\n","test\t Loss: 0.7926\t Acc: 88.6700%\n","-------------------------------------------\n","\n","Epoch 150/199\n","----------\n"]}]},{"cell_type":"code","source":["# Best test Acc: 88.180000%\n","# Epoch 100/199"],"metadata":{"id":"bVoOfzx4UMun"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZnXG25KVBQeQ"},"execution_count":null,"outputs":[]}]}