{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClassDropout(nn.Module):\n",
    "    def __init__(self, num_classes, p=0.5):\n",
    "        super(ClassDropout, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if self.training:\n",
    "            batch_size, num_features = input.size()\n",
    "            dropout_mask = torch.ones_like(input)\n",
    "\n",
    "            for i in range(self.num_classes):\n",
    "                class_size = num_features // self.num_classes\n",
    "                start = i * class_size\n",
    "                end = (i + 1) * class_size\n",
    "\n",
    "                # Generate dropout mask for each class\n",
    "                mask = torch.zeros((batch_size, class_size))\n",
    "                mask[labels == i, :class_size//10] = 1\n",
    "\n",
    "                dropout_mask[:, start:end] *= mask\n",
    "\n",
    "            output = input * dropout_mask\n",
    "        else:\n",
    "            output = input\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dropout = ClassDropout(num_classes=2, p=0.5)\n",
    "x = torch.randn(10, 4)\n",
    "labels = torch.randint(0, 4, (10,))\n",
    "output = dropout(x, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., -0., 0., 0.],\n        [0., 0., -0., 0.],\n        [0., 0., 0., 0.],\n        [-0., 0., 0., -0.],\n        [0., -0., 0., -0.],\n        [-0., 0., 0., -0.],\n        [-0., 0., 0., -0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [-0., -0., 0., 0.]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 1, 2, 0, 0, 2, 1, 1, 0, 0])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LabelDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(LabelDropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if self.training:\n",
    "            dropout_mask = torch.ones_like(input, dtype=torch.float32)  # Convert dropout_mask to float32\n",
    "\n",
    "            # Generate dropout mask based on label\n",
    "            mask = torch.zeros_like(labels, dtype=torch.float32)  # Convert mask to float32\n",
    "            mask[labels == 1] = torch.bernoulli(torch.ones_like(labels[labels == 1]).float() * self.p)\n",
    "\n",
    "            dropout_mask *= mask.unsqueeze(1)\n",
    "\n",
    "            output = input * dropout_mask\n",
    "        else:\n",
    "            output = input\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dropout = LabelDropout(p=0.5)\n",
    "x = torch.randn(10, 2)\n",
    "labels = torch.randint(0, 2, (10,))\n",
    "output = dropout(x, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000, -0.0000],\n        [-0.0000,  0.0000],\n        [ 0.0000, -0.0000],\n        [ 0.0000,  0.0000],\n        [-0.0000, -0.0000],\n        [ 1.0261,  0.9428],\n        [ 0.0000,  0.0000],\n        [-0.6378, -0.9000],\n        [ 1.2907, -0.1097],\n        [-0.0000,  0.0000]])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1459, -0.0000],\n        [ 4.7492, -0.0000],\n        [-0.0000,  0.9335],\n        [ 0.0000, -0.0000],\n        [-0.0000, -0.1842],\n        [-0.0000,  0.0000],\n        [-0.0000, -0.0672],\n        [ 0.8640, -2.0705],\n        [ 0.0000, -2.4501],\n        [-5.7576, -0.0000]])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# Example usage\n",
    "output = dropout(x)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LabelDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(LabelDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.item = 0\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        if self.training:\n",
    "            if self.item == len(labels) -1:\n",
    "                self.item = 0 # end of forward\n",
    "            self.item += 1\n",
    "            dropout_mask = torch.ones_like(input, dtype=torch.float32)  # Convert dropout_mask to float32\n",
    "\n",
    "            # Generate dropout mask based on label\n",
    "            mask = torch.zeros_like(dropout_mask, dtype=torch.float32)  # Convert mask to float32\n",
    "\n",
    "            # Turn off the 0th neuron for label 0\n",
    "            mask[labels == 0, 0] = 1.0 - self.p\n",
    "\n",
    "            # Turn off the 1st neuron for label 1\n",
    "            mask[labels == 1, 1] = 1.0 - self.p\n",
    "\n",
    "            dropout_mask *= mask\n",
    "\n",
    "            output = input * dropout_mask\n",
    "        else:\n",
    "            output = input\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "x = torch.randn(10, 2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000, -0.3690],\n        [ 0.0000, -0.2702],\n        [ 0.0000, -1.0179],\n        [ 0.0000,  0.2240],\n        [-0.0000,  0.2196],\n        [-0.7385,  0.0000],\n        [ 0.0334,  0.0000],\n        [ 0.0000,  0.2228],\n        [ 0.0000, -0.5198],\n        [-0.3733, -0.0000]])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "dropout = LabelDropout(p=0.5)\n",
    "labels = torch.randint(0, 2, (10,))\n",
    "output = dropout(x, labels)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear = nn.Linear(16 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        print(labels)\n",
    "        batch_size = input.size(0)\n",
    "        print(batch_size)\n",
    "        output = self.conv(input)\n",
    "        print(output.shape)\n",
    "        output = self.maxpool(output)\n",
    "        print(output.shape)\n",
    "        output = output.view(batch_size, -1)  # Reshape output to [batch_size, num_features]\n",
    "        print(output.shape)\n",
    "        output = self.linear(output)\n",
    "        print(output.shape)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the module\n",
    "module = MyModule()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Generate a batch of input tensors\n",
    "input_batch = torch.randn(64, 1, 28, 28)  # Batch size of 64, input with 1 channel, 28x28 size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 0, 1, 9, 4, 5, 3, 1, 2, 6, 6, 3, 5, 4, 7, 7, 8, 5, 2, 3, 8, 4, 3, 2,\n",
      "        4, 2, 2, 0, 2, 0, 2, 1, 4, 7, 8, 2, 8, 1, 4, 5, 2, 8, 3, 5, 9, 1, 1, 2,\n",
      "        3, 0, 4, 1, 6, 6, 7, 6, 6, 4, 4, 9, 0, 1, 1, 8])\n",
      "64\n",
      "torch.Size([64, 16, 28, 28])\n",
      "torch.Size([64, 16, 14, 14])\n",
      "torch.Size([64, 3136])\n",
      "torch.Size([64, 10])\n",
      "tensor([[ 0.1822,  0.0377, -0.2974,  0.0109, -0.8199,  0.2558, -0.4759, -0.3422,\n",
      "         -0.2189,  0.1247],\n",
      "        [ 0.5903,  0.5092,  0.1869, -0.3818, -0.0939,  0.0187, -0.4818,  0.0236,\n",
      "         -0.5225,  0.0729],\n",
      "        [ 0.1212,  0.7494, -0.2593, -0.1949, -0.8086, -0.1106, -0.3424, -0.1479,\n",
      "         -0.1246, -0.5434],\n",
      "        [ 0.2075,  0.4368,  0.2989, -0.8117, -0.6151,  0.2387, -0.7075, -0.3671,\n",
      "         -0.4976, -0.0128],\n",
      "        [ 0.1929, -0.0650, -0.1420, -0.3704, -0.5443,  0.3669, -0.8789, -0.3265,\n",
      "         -1.0529,  0.3100],\n",
      "        [ 0.5688,  0.0724, -0.1103, -0.4128, -0.6750,  0.2666, -0.6940, -0.1717,\n",
      "         -0.7910,  0.2909],\n",
      "        [ 0.0706,  0.3235, -0.2637,  0.0394, -0.4618,  0.0629, -0.4021, -0.4186,\n",
      "         -0.3585,  0.4787],\n",
      "        [ 0.2389,  0.4632,  0.2548, -0.3515, -0.1647, -0.0267, -0.9866, -0.4346,\n",
      "         -0.6363,  0.2028],\n",
      "        [ 0.3668, -0.0155,  0.2004, -0.1139, -0.5300,  0.3177, -0.3801, -0.1615,\n",
      "         -0.4482,  0.1534],\n",
      "        [ 0.6554, -0.0322, -0.2467, -0.1723, -0.8276,  0.1565, -0.4522, -0.1812,\n",
      "         -0.7235,  0.0193],\n",
      "        [ 0.1885,  0.2092, -0.5207, -0.0751, -0.5274,  0.2588, -0.4303, -0.0144,\n",
      "         -0.3957,  0.3035],\n",
      "        [ 0.1723,  0.3365, -0.1388, -0.3296, -0.2494,  0.3852, -0.1724, -0.0722,\n",
      "         -0.1544,  0.3059],\n",
      "        [ 0.3080,  0.3106, -0.4486, -0.4720, -0.3669,  0.4782, -0.4282, -0.0252,\n",
      "         -0.7705, -0.0149],\n",
      "        [ 0.4943, -0.0464,  0.0049, -0.6297, -0.5396,  0.4793, -0.4470,  0.1076,\n",
      "         -0.7164,  0.0956],\n",
      "        [ 0.0390,  0.2025, -0.0699, -0.3804, -0.3281, -0.0380, -0.5021, -0.1248,\n",
      "         -0.0832, -0.0182],\n",
      "        [ 0.0748,  0.3844,  0.1510, -0.3419, -0.5201,  0.1801, -0.1413, -0.2642,\n",
      "         -0.3067,  0.0074],\n",
      "        [ 0.2289,  0.3927, -0.0304, -0.3052, -0.6079,  0.1959, -0.1524, -0.3897,\n",
      "         -0.3459,  0.1838],\n",
      "        [ 0.4593,  0.1777,  0.1585, -0.4698, -0.4628,  0.1773, -0.2604,  0.2197,\n",
      "         -0.5808, -0.0384],\n",
      "        [ 0.3351,  0.3051, -0.1122, -0.0767, -0.8581,  0.2067, -0.5799, -0.3920,\n",
      "         -0.3457,  0.0484],\n",
      "        [ 0.3407,  0.2074, -0.1821, -0.3417, -0.6181,  0.4206, -0.9566, -0.0987,\n",
      "         -0.3038,  0.0310],\n",
      "        [ 0.3501,  0.0022, -0.0225, -0.8844, -0.4942,  0.3026, -0.3753, -0.3246,\n",
      "         -0.1909,  0.2277],\n",
      "        [ 0.1033,  0.6513, -0.1996, -0.4483,  0.0077,  0.1159, -0.5414, -0.3781,\n",
      "         -0.2429,  0.5627],\n",
      "        [ 0.2615,  0.1362,  0.1898, -0.4505, -0.7019,  0.2283, -0.7354, -0.1061,\n",
      "         -0.6381, -0.1280],\n",
      "        [ 0.0251,  0.0705, -0.0919,  0.1626, -0.6663, -0.1011, -0.5358,  0.0127,\n",
      "         -0.1418,  0.2467],\n",
      "        [ 0.1877, -0.0550, -0.0709, -0.5329, -0.5822, -0.0956, -0.3063, -0.1142,\n",
      "         -0.6029,  0.1459],\n",
      "        [ 0.4310,  0.1976,  0.0489, -0.2533, -0.3615,  0.2254, -0.2706,  0.3109,\n",
      "         -0.3748,  0.0975],\n",
      "        [ 0.0307,  0.2059, -0.0510, -0.5137, -0.3260,  0.6749, -0.4786, -0.3279,\n",
      "         -0.6581,  0.1643],\n",
      "        [ 0.5746,  0.5338,  0.0427, -0.4168, -1.1331,  0.1624, -0.5438, -0.1359,\n",
      "         -0.6555,  0.0198],\n",
      "        [ 0.3439,  0.5208, -0.5308, -0.4019, -1.1931,  0.6728, -0.4257, -0.1609,\n",
      "         -0.3255,  0.2790],\n",
      "        [ 0.2129,  0.0369, -0.2820, -0.3666, -0.8520,  0.2422, -0.7081, -0.2801,\n",
      "         -0.0422,  0.3080],\n",
      "        [ 0.4644,  0.3746,  0.0279, -0.2899, -0.1732,  0.3513, -0.2357, -0.1573,\n",
      "         -0.3020,  0.3449],\n",
      "        [ 0.5487,  0.1800, -0.0941, -0.4290, -0.6829,  0.4665, -0.2613,  0.0682,\n",
      "         -0.6190,  0.2833],\n",
      "        [ 0.1510, -0.1725, -0.4201, -0.2814, -0.1537,  0.5810, -0.4008,  0.2044,\n",
      "         -0.3616,  0.0429],\n",
      "        [ 0.0095,  0.0634, -0.4510, -0.6170, -0.3682,  0.2667, -0.3374,  0.0936,\n",
      "         -0.3318, -0.0050],\n",
      "        [ 0.2630,  0.1714, -0.2658, -0.0581, -0.8218,  0.1533, -0.2652,  0.0818,\n",
      "         -0.4915,  0.3147],\n",
      "        [ 0.1367,  0.2133, -0.2160, -0.0277, -0.6427, -0.2794, -0.7009,  0.0838,\n",
      "         -0.4281,  0.3539],\n",
      "        [ 0.1243,  0.1842,  0.1711, -0.2784, -0.5517,  0.2609, -0.4233,  0.4325,\n",
      "         -0.4628, -0.0368],\n",
      "        [ 0.4084,  0.0374, -0.5166, -0.5405, -0.6483,  0.2488, -0.3384, -0.2649,\n",
      "         -0.6697,  0.1426],\n",
      "        [ 0.0457, -0.1648, -0.3460, -0.4154, -0.4293,  0.1179, -0.4354, -0.2258,\n",
      "         -0.5402,  0.2700],\n",
      "        [-0.1453,  0.2742, -0.3932, -0.1484, -0.4320,  0.2775, -0.7685, -0.2765,\n",
      "         -0.6818,  0.0807],\n",
      "        [ 0.3267,  0.0414, -0.0761, -0.6717, -0.5737,  0.2190, -0.4400, -0.2294,\n",
      "         -0.2643, -0.1440],\n",
      "        [ 0.2320,  0.2833, -0.0845, -0.4357, -0.6745,  0.3228, -0.3637,  0.0059,\n",
      "         -0.3985, -0.0897],\n",
      "        [ 0.7285,  0.8167,  0.0020, -0.3832, -0.5639,  0.3197, -0.7754, -0.0722,\n",
      "         -0.4668,  0.3442],\n",
      "        [-0.3495,  0.6017,  0.2989, -0.4093, -0.6919,  0.5439, -0.3544,  0.1134,\n",
      "         -0.1421,  0.2525],\n",
      "        [ 0.3906,  0.2144, -0.0832, -0.5558, -0.3363,  0.3759, -0.4134,  0.0927,\n",
      "         -0.7990,  0.0845],\n",
      "        [ 0.3345,  0.2632,  0.3515, -0.4662, -0.0082,  0.3309, -0.5229, -0.3749,\n",
      "         -0.5059,  0.3514],\n",
      "        [ 0.4026,  0.3894, -0.4194,  0.2976, -0.8348,  0.0058, -0.3380, -0.2422,\n",
      "         -0.4093, -0.1491],\n",
      "        [ 0.4422,  0.1241, -0.2309, -0.6180, -0.8230,  0.5302, -0.4299,  0.2076,\n",
      "         -0.8041, -0.0095],\n",
      "        [-0.0568, -0.2650, -0.4449, -0.4622, -0.7119,  0.4185, -0.4345,  0.3509,\n",
      "         -0.4514, -0.2386],\n",
      "        [ 0.1245,  0.0659, -0.2516, -0.9154, -0.3320,  0.4257, -0.7462, -0.1111,\n",
      "         -0.7513,  0.1714],\n",
      "        [ 0.4548,  0.1765,  0.2632, -0.4683, -0.8182,  0.1476, -0.6183, -0.0771,\n",
      "         -0.2781, -0.1569],\n",
      "        [-0.1019,  0.3409,  0.1478, -0.2291, -0.6294,  0.0370, -0.2560,  0.0701,\n",
      "         -0.6049,  0.1867],\n",
      "        [ 0.3331,  0.1736, -0.1093, -0.4561, -0.7399,  0.6355, -0.2731,  0.1979,\n",
      "         -0.3115,  0.3579],\n",
      "        [-0.2768,  0.7610, -0.2759, -0.8001, -0.5629,  0.4811, -0.5069, -0.2869,\n",
      "         -0.5772, -0.1910],\n",
      "        [ 0.4194, -0.2990,  0.0385, -0.0368, -0.3643,  0.4572, -0.8978,  0.0662,\n",
      "         -0.2090, -0.4766],\n",
      "        [ 0.5055,  0.1677,  0.1249, -0.6190, -0.5294, -0.3211, -0.3187, -0.0127,\n",
      "         -0.2785,  0.2272],\n",
      "        [ 0.5556,  0.8259, -0.3398, -0.3763, -0.5071,  0.3391, -0.2808, -0.3241,\n",
      "         -0.9425, -0.0678],\n",
      "        [ 0.4673,  0.1189,  0.0574, -0.1496, -0.5969, -0.0372, -1.0598,  0.3677,\n",
      "         -0.5475, -0.1796],\n",
      "        [ 0.0760, -0.0506, -0.2654, -0.5241, -0.8082,  0.3244, -0.5373,  0.0505,\n",
      "         -0.5385, -0.1615],\n",
      "        [ 0.0947,  0.3898, -0.2578, -0.4872, -0.4217,  0.4411, -0.2399, -0.3122,\n",
      "         -0.3644,  0.0104],\n",
      "        [ 0.2831,  0.2509, -0.0977, -0.5640, -0.5687,  0.2309, -0.4544, -0.4406,\n",
      "         -0.3719,  0.0475],\n",
      "        [ 0.3571,  0.5116, -0.1000, -0.5208, -0.6564,  0.0346, -0.9048, -0.0743,\n",
      "         -0.4702, -0.0018],\n",
      "        [ 0.0250,  0.2653, -0.2265, -0.4594, -0.4947,  0.3579, -0.8047,  0.2084,\n",
      "         -0.4169, -0.0582],\n",
      "        [ 0.0335,  0.1414,  0.1625, -0.4054, -0.2329, -0.2990, -0.5494, -0.1452,\n",
      "         -0.0714,  0.0106]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([64, 10])"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the input batch through the module\n",
    "labels = torch.randint(low=0, high=10, size=(64,))\n",
    "\n",
    "output_batch = module(input_batch, labels)\n",
    "print(output_batch)\n",
    "output_batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "tensor([[-6.4237e-01, -2.4918e-01, -4.6867e-01,  8.4640e-01,  3.3323e-01],\n",
      "        [-7.6395e-01, -4.7019e-01, -1.6407e-01,  1.5515e+00,  5.0242e-01],\n",
      "        [-8.3392e-02, -4.2231e-03, -5.0815e-02,  8.0905e-02, -9.5689e-01],\n",
      "        [ 4.4424e-01,  6.3291e-01,  6.5886e-01, -7.0136e-01, -8.7413e-01],\n",
      "        [-2.7180e-01, -6.2154e-01,  8.1393e-02,  2.7866e-01, -3.5610e-01],\n",
      "        [ 2.2319e-01, -1.4222e-02, -9.3739e-02, -3.0192e-01, -1.1041e+00],\n",
      "        [-1.7145e-01,  5.0058e-01, -7.9980e-02, -3.4856e-01, -7.1968e-01],\n",
      "        [-2.8983e-01, -4.5293e-01, -3.6488e-01,  7.1441e-01, -1.7725e-01],\n",
      "        [ 7.6591e-01,  7.4476e-01,  9.1621e-01, -1.2377e+00, -8.4511e-01],\n",
      "        [ 6.5633e-01, -2.4939e-01, -2.9857e-01,  9.7941e-02, -8.9881e-01],\n",
      "        [-2.6623e-01,  1.3860e-01,  1.7732e-01,  4.4850e-01, -7.5409e-02],\n",
      "        [ 6.3799e-01,  3.7170e-01, -3.3426e-01,  1.1796e+00, -5.8039e-01],\n",
      "        [ 6.5858e-01,  2.4646e-01,  1.0536e+00,  5.8700e-01, -1.6025e-01],\n",
      "        [ 8.4177e-01,  3.7622e-01,  6.2475e-01, -1.1494e+00, -1.0257e+00],\n",
      "        [-1.0081e-01,  3.1711e-01, -9.3893e-02,  8.8965e-01, -2.1436e-01],\n",
      "        [ 1.3982e+00,  5.5969e-01,  2.5485e-01, -6.8556e-01, -8.0631e-01],\n",
      "        [ 7.6051e-01,  1.2190e-01,  6.1698e-01, -1.8351e-01, -3.8006e-01],\n",
      "        [-8.9496e-02, -6.6251e-01, -3.4584e-01, -8.4152e-01, -3.1799e-01],\n",
      "        [ 1.5385e-01,  7.2835e-01,  9.8931e-01, -1.0138e+00, -2.6188e-01],\n",
      "        [-5.9050e-02,  2.3150e-01,  1.8883e-01,  2.5024e-01,  3.4203e-02],\n",
      "        [ 7.2741e-01, -2.4544e-01,  8.0691e-02,  8.3667e-01, -1.6153e-01],\n",
      "        [ 2.8845e-01, -2.8400e-01,  4.2623e-02,  1.3712e+00,  2.3069e-01],\n",
      "        [ 5.4912e-03, -2.7515e-01, -5.1905e-01,  1.0449e+00,  1.5348e-01],\n",
      "        [ 6.8006e-01, -3.1976e-01,  3.8700e-01, -1.5097e+00, -6.6442e-01],\n",
      "        [-2.5522e-01,  3.3358e-01,  1.1218e+00,  9.2552e-01, -7.7957e-01],\n",
      "        [-6.3016e-01, -1.7096e-01,  1.0606e-01, -7.0787e-01, -5.5043e-01],\n",
      "        [ 6.6959e-01, -2.9850e-01,  1.5582e-01, -1.0470e+00, -4.6264e-01],\n",
      "        [ 2.0802e-01,  1.7752e-01,  1.9188e-01,  9.3368e-01, -4.0377e-01],\n",
      "        [ 4.4510e-01,  4.2651e-01, -1.7407e-01, -1.3716e+00, -1.6361e+00],\n",
      "        [-1.7066e-01, -1.1084e-02,  3.0877e-01,  4.7268e-01,  1.5016e-01],\n",
      "        [-1.0617e-01,  7.7085e-01,  1.2370e+00, -4.2719e-01, -5.4575e-01],\n",
      "        [ 8.5092e-01,  9.8095e-02, -9.5931e-02, -8.0546e-01, -6.8641e-01],\n",
      "        [ 8.0443e-01, -7.1487e-02,  5.1505e-01, -6.1720e-01, -6.3358e-01],\n",
      "        [ 1.1124e+00,  2.3710e-01,  5.2870e-01, -4.4652e-01, -5.3679e-01],\n",
      "        [ 8.2445e-01, -4.4074e-02,  7.9377e-01, -2.9073e-01, -4.3460e-01],\n",
      "        [-1.0785e+00, -4.4675e-01, -4.5496e-01,  5.0407e-01, -2.5622e-01],\n",
      "        [ 3.1770e-01, -1.2570e-01, -1.2727e-01,  9.0905e-01, -7.7010e-02],\n",
      "        [ 6.4146e-01,  2.1159e-02,  3.9790e-01,  3.1469e-01, -3.6318e-01],\n",
      "        [ 4.8446e-01, -2.2647e-01,  2.8723e-01,  7.0831e-01,  3.1854e-01],\n",
      "        [ 2.5291e-01, -5.7096e-01,  6.6958e-01,  2.1240e+00,  7.8644e-01],\n",
      "        [ 8.0979e-01,  5.6480e-01,  2.6220e-01, -3.0470e-01, -6.3043e-01],\n",
      "        [ 1.2170e+00,  2.9311e-01,  6.9327e-01, -1.2784e+00, -4.9125e-01],\n",
      "        [-7.2159e-01, -1.5661e-01, -1.4934e+00, -1.7587e-01, -8.0201e-01],\n",
      "        [ 2.2983e-01,  4.0752e-03,  1.0668e-01,  8.9633e-01, -6.0330e-02],\n",
      "        [ 5.9937e-01,  8.2632e-01,  1.2514e+00, -8.0599e-02, -9.0892e-01],\n",
      "        [ 3.2874e-01,  1.1608e-03,  6.8197e-01, -7.8114e-01, -2.8759e-01],\n",
      "        [ 3.4170e-01,  1.7314e-01,  6.5832e-01, -2.6526e-01, -4.3664e-01],\n",
      "        [-4.1856e-01,  2.1701e-01,  1.9218e-01, -3.2727e-01, -4.2371e-01],\n",
      "        [ 9.5635e-01,  3.1747e-01,  8.0993e-01, -3.5465e-01, -5.7313e-01],\n",
      "        [ 9.1503e-01,  2.6566e-01,  4.5220e-01,  4.7491e-02,  6.7698e-01],\n",
      "        [ 7.1907e-01,  6.6593e-01,  5.1643e-01, -5.7212e-01, -5.0963e-01],\n",
      "        [-7.9912e-01, -1.2906e-01,  3.0309e-02,  1.6924e+00,  5.0584e-02],\n",
      "        [ 1.0922e+00, -2.3146e-01,  3.7926e-01,  5.2168e-01,  5.6321e-01],\n",
      "        [ 2.4976e-03,  2.3913e-02,  2.8271e-01,  6.8087e-01,  8.6596e-02],\n",
      "        [ 3.1209e-01,  5.8601e-01, -1.1906e-01, -4.8676e-01, -1.6958e+00],\n",
      "        [-3.3286e-01, -1.6608e-01,  4.8143e-01, -6.2763e-02, -3.5690e-01],\n",
      "        [ 1.3783e+00, -3.6669e-01,  2.5535e-02,  2.5694e-01,  6.7928e-01],\n",
      "        [ 8.8325e-01,  2.5496e-01,  2.9089e-01, -6.7674e-01, -9.5339e-01],\n",
      "        [-2.1796e-01, -3.0597e-01, -1.1222e-01, -7.5642e-01, -1.1284e+00],\n",
      "        [ 1.6251e-01, -5.1356e-02,  9.5531e-02, -1.5757e-01, -5.9842e-01],\n",
      "        [ 5.2332e-01,  3.0301e-01,  3.6126e-01,  7.4706e-01, -5.1219e-01],\n",
      "        [-1.8043e-01, -2.9401e-01, -1.5131e-01,  1.4751e+00, -1.1331e-01],\n",
      "        [ 4.3155e-01, -1.1890e-01,  2.7041e-01,  5.8564e-01, -8.3614e-02],\n",
      "        [ 8.4125e-01,  1.0627e-01,  6.3475e-01, -7.1802e-02, -1.2226e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, input, arg1, arg2):\n",
    "        print(arg2)\n",
    "        # Use arg1 and arg2 in the forward computation\n",
    "        output = self.linear(input)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the module\n",
    "module = MyModule()\n",
    "\n",
    "# Generate a batch of input tensors\n",
    "input_batch = torch.randn(64, 10)  # Batch size of 64\n",
    "\n",
    "# Pass the input batch and additional arguments through the module\n",
    "arg1 = 1\n",
    "arg2 = 'hello'\n",
    "output_batch = module(input_batch, arg1, arg2)\n",
    "\n",
    "print(output_batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "tensor(1.9145, grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0000, 0.0000, -0.0000, 0.5852, 0.1209, 0.4885],\n",
      "        [-0.0000, -0.0000, -0.0000, 1.0173, 0.0433, 0.5012],\n",
      "        [0.7642, 0.7154, 0.2257, -0.0000, 0.0000, 0.0000],\n",
      "        [0.1500, 0.2710, 0.2988, -0.0000, -0.0000, 0.0000],\n",
      "        [0.5449, 0.1743, 0.1416, -0.0000, 0.0000, -0.0000],\n",
      "        [0.0000, -0.0000, 0.0000, 0.7141, 0.2616, 1.0926]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([1, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(10, 6)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        output = self.linear(input)\n",
    "\n",
    "        # Compute loss using the predictions and true labels\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "\n",
    "        # Apply custom dropout operation based on the labels\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            # Todo: need to pass the right number of labels\n",
    "            output[i] = custom_dropout(output[i], self.p, label=label, num_of_labels=2)\n",
    "            # Add more conditions for other labels if needed\n",
    "\n",
    "        return output, loss\n",
    "\n",
    "def custom_dropout(tensor, p, label, num_of_labels):\n",
    "    if p > 0 and tensor.dim() > 0:\n",
    "        mask = torch.bernoulli(torch.ones_like(tensor) * (1 - p))\n",
    "        num_elements = mask.numel()\n",
    "        portion_size = int(num_elements // num_of_labels)\n",
    "        mask[label*portion_size:(label+1)*portion_size] = 1.0\n",
    "        return tensor * mask\n",
    "    return tensor\n",
    "\n",
    "# Create an instance of the module\n",
    "module = MyModule(p=1)\n",
    "\n",
    "# Generate a batch of input tensors and labels\n",
    "input_batch = torch.randn(6, 10)  # Batch size of 64\n",
    "labels = torch.randint(0, 2, (6,))  # Random labels\n",
    "\n",
    "# Pass the input batch and labels through the module\n",
    "output_batch, loss = module(input_batch, labels)\n",
    "\n",
    "print(loss)\n",
    "print(output_batch)\n",
    "print(labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    :parameter\n",
    "    p: probability to drop. Bigger p -> Drop more\n",
    "    \"\"\"\n",
    "    def __init__(self, num_of_labels, p=0.5):\n",
    "        super(CustomDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.num_of_labels = num_of_labels\n",
    "\n",
    "    def forward(self, batch_input, batch_labels):\n",
    "        if self.training:\n",
    "            layer_size = batch_input.size(1)\n",
    "            batch_size = batch_input.size(0)\n",
    "            portion_size = int(layer_size // self.num_of_labels)\n",
    "            print(f\"portion_size: {portion_size}\")\n",
    "            print(batch_input)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                label = batch_labels[i].item()\n",
    "                # print(f\"{label} * {portion_size} = {label * portion_size}\")\n",
    "                # print(f\"{label+1} * {portion_size} = {(label + 1) * portion_size}\")\n",
    "                mask = torch.bernoulli(torch.ones_like(batch_input[i]) * (1 - self.p))\n",
    "                # mask = torch.zeros_like()\n",
    "                # print(label * portion_size, \"-\", (label + 1) * portion_size)\n",
    "                print((1.0 + (1/portion_size)))\n",
    "                mask[label * portion_size : (label + 1) * portion_size] = self.num_of_labels  # give each neuron power of the dropped neurons\n",
    "                batch_input[i] = batch_input[i] * mask\n",
    "\n",
    "            output = batch_input\n",
    "        else:\n",
    "            output = batch_input\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    \"\"\"\n",
    "    big p -> drop more\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linear = nn.Linear(10, 3)\n",
    "        self.dropout = CustomDropout(3, p)\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        output = self.linear(input)\n",
    "        # Apply custom dropout operation based on the labels\n",
    "        output = self.dropout(output, labels)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portion_size: 1\n",
      "tensor([[ 0.5711, -0.5323,  0.1424],\n",
      "        [ 0.1427, -0.2607, -0.0401],\n",
      "        [ 0.8352, -0.4630, -1.0318],\n",
      "        [-0.0449,  0.1558,  0.4080],\n",
      "        [ 0.3593, -0.5124, -0.2071],\n",
      "        [-0.6994, -0.0364,  0.4330]], grad_fn=<AddmmBackward0>)\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[ 0.0000, -0.0000,  0.4272],\n         [ 0.0000, -0.7821, -0.0000],\n         [ 0.0000, -1.3891, -0.0000],\n         [-0.0000,  0.0000,  1.2241],\n         [ 0.0000, -0.0000, -0.6214],\n         [-0.0000, -0.0000,  1.2991]], grad_fn=<CopySlices>),\n tensor([2, 1, 1, 2, 2, 2]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the module\n",
    "module = MyModule(p=1)\n",
    "\n",
    "# Generate a batch of input tensors and labels\n",
    "input_batch = torch.randn(6, 10)  # Batch size of 6\n",
    "\n",
    "labels = torch.randint(0, 3, (6,))  # Random labels\n",
    "\n",
    "# Pass the input batch and labels through the module\n",
    "output_batch = module(input_batch, labels)\n",
    "\n",
    "\n",
    "output_batch, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.4066666666666667"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.2200/3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Todo:\n",
    "* need to add power in relation to p. now p is 1 so add power of ALL other neurons"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}